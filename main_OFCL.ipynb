{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from configuration import config_jup\n",
    "from utils.data_loader import get_loader_all_clients\n",
    "from utils.train_utils import get_logger, initialize_clients, FedAvg, weightedFedAvg, test_global_model, save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = config_jup.base_parser()\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "    args.device = f'cuda:0'\n",
    "else:\n",
    "    args.device = 'cpu' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(framework='FCL', dir_data='./data/', dir_output='./output/', dataset_name='cifar10', model_name='resnet', batch_size=10, lr=0.1, optimizer='sgd', local_epochs=1, n_runs=1, n_tasks=5, with_memory=1, memory_size=500, update_strategy='balanced', sampling_strategy='random', balanced_update='random', uncertainty_score='bregman', subsample_size=50, balanced_step='bottomk', n_clients=5, overlap='non-overlap', burnin=30, jump=5, fl_update='w_favg', cuda=True, device='cuda:0', input_size=(3, 32, 32), n_classes=10, n_classes_per_task=2, dir_results='./output//FCL/cifar10/w_favg/non-overlap/30/5/resnet/sgd/01/500/10/1/random/balanced_random/')\n"
     ]
    }
   ],
   "source": [
    "args.n_runs = 1\n",
    "logger = get_logger(args)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 - Client 0 - Task 0 completed - (2, 9)\n",
      "Run 0 - Client 0 - Test time - Task 0\n",
      "Run 0 - Client 1 - Task 0 completed - (4, 1)\n",
      "Run 0 - Client 1 - Test time - Task 0\n",
      "Run 0 - Client 2 - Task 0 completed - (5, 4)\n",
      "Run 0 - Client 2 - Test time - Task 0\n",
      "Run 0 - Client 3 - Task 0 completed - (3, 8)\n",
      "Run 0 - Client 3 - Test time - Task 0\n",
      "Run 0 - Client 4 - Task 0 completed - (9, 5)\n",
      "Run 0 - Client 4 - Test time - Task 0\n",
      "Run 0 - Client 0 - Task 1 completed - (6, 4)\n",
      "Run 0 - Client 0 - Test time - Task 1\n",
      "Run 0 - Client 1 - Task 1 completed - (5, 0)\n",
      "Run 0 - Client 1 - Test time - Task 1\n",
      "Run 0 - Client 2 - Task 1 completed - (1, 2)\n",
      "Run 0 - Client 2 - Test time - Task 1\n",
      "Run 0 - Client 3 - Task 1 completed - (4, 9)\n",
      "Run 0 - Client 3 - Test time - Task 1\n",
      "Run 0 - Client 4 - Task 1 completed - (2, 4)\n",
      "Run 0 - Client 4 - Test time - Task 1\n",
      "Run 0 - Client 0 - Task 2 completed - (0, 3)\n",
      "Run 0 - Client 0 - Test time - Task 2\n",
      "Run 0 - Client 1 - Task 2 completed - (7, 2)\n",
      "Run 0 - Client 1 - Test time - Task 2\n",
      "Run 0 - Client 2 - Task 2 completed - (9, 6)\n",
      "Run 0 - Client 2 - Test time - Task 2\n",
      "Run 0 - Client 3 - Task 2 completed - (2, 6)\n",
      "Run 0 - Client 3 - Test time - Task 2\n",
      "Run 0 - Client 4 - Task 2 completed - (7, 1)\n",
      "Run 0 - Client 4 - Test time - Task 2\n",
      "Run 0 - Client 0 - Task 3 completed - (1, 7)\n",
      "Run 0 - Client 0 - Test time - Task 3\n",
      "Run 0 - Client 1 - Task 3 completed - (3, 6)\n",
      "Run 0 - Client 1 - Test time - Task 3\n",
      "Run 0 - Client 2 - Task 3 completed - (7, 0)\n",
      "Run 0 - Client 2 - Test time - Task 3\n",
      "Run 0 - Client 3 - Task 3 completed - (0, 1)\n",
      "Run 0 - Client 3 - Test time - Task 3\n",
      "Run 0 - Client 4 - Task 3 completed - (0, 8)\n",
      "Run 0 - Client 4 - Test time - Task 3\n",
      "Run 0 - Client 0 - Task 4 completed - (8, 5)\n",
      "Run 0 - Client 0 - Test time - Task 4\n",
      "Run 0 - Client 0 - Train completed\n",
      "Run 0 - Client 1 - Task 4 completed - (9, 8)\n",
      "Run 0 - Client 1 - Test time - Task 4\n",
      "Run 0 - Client 1 - Train completed\n",
      "Run 0 - Client 2 - Task 4 completed - (3, 8)\n",
      "Run 0 - Client 2 - Test time - Task 4\n",
      "Run 0 - Client 2 - Train completed\n",
      "Run 0 - Client 3 - Task 4 completed - (5, 7)\n",
      "Run 0 - Client 3 - Test time - Task 4\n",
      "Run 0 - Client 3 - Train completed\n",
      "Run 0 - Client 4 - Task 4 completed - (6, 3)\n",
      "Run 0 - Client 4 - Test time - Task 4\n",
      "Run 0 - Client 4 - Train completed\n"
     ]
    }
   ],
   "source": [
    "for run in range(args.n_runs):\n",
    "    loader_clients, cls_assignment_list, global_test_loader = get_loader_all_clients(args, run)\n",
    "    clients = initialize_clients(args, loader_clients, cls_assignment_list, run)\n",
    "\n",
    "    while not all([client.train_completed for client in clients]):\n",
    "        for client in clients:\n",
    "            if not client.train_completed:\n",
    "                samples, labels = client.get_next_batch()\n",
    "\n",
    "                if samples is not None:\n",
    "                    if args.with_memory:\n",
    "                        if client.task_id == 0:\n",
    "                            client.train_with_update(samples, labels)\n",
    "                        else:\n",
    "                            client.train_with_memory(samples, labels)\n",
    "                    else:\n",
    "                        client.train(samples, labels)\n",
    "\n",
    "                else:\n",
    "                    print(f'Run {run} - Client {client.client_id} - Task {client.task_id} completed - {client.get_current_task()}')\n",
    "                    # compute loss train\n",
    "                    logger = client.compute_loss(logger, run)\n",
    "                    print(f'Run {run} - Client {client.client_id} - Test time - Task {client.task_id}')\n",
    "                    logger = client.test(logger, run)\n",
    "                    logger = client.validation(logger, run)\n",
    "                    logger = client.forgetting(logger, run)\n",
    "\n",
    "                    if client.task_id + 1 >= args.n_tasks:\n",
    "                        client.train_completed = True\n",
    "                        print(f'Run {run} - Client {client.client_id} - Train completed')\n",
    "                        logger = client.balanced_accuracy(logger, run)\n",
    "                    else:\n",
    "                        client.task_id += 1\n",
    "\n",
    "        # COMMUNICATION ROUND PART\n",
    "        selected_clients = [client.client_id for client in clients if (client.num_batches >= args.burnin and client.num_batches % args.jump == 0 and client.train_completed == False)]\n",
    "        if len(selected_clients) > 1:\n",
    "            # communication round when all clients process a mini-batch\n",
    "            if args.fl_update == 'favg':\n",
    "                global_model = FedAvg(args, selected_clients, clients)\n",
    "            if args.fl_update == 'w_favg':\n",
    "                global_model = weightedFedAvg(args, selected_clients, clients)\n",
    "\n",
    "            global_parameters = global_model.state_dict()\n",
    "            # local models update with averaged global parameters\n",
    "            for client_id in selected_clients:\n",
    "                clients[client_id].update_parameters(global_parameters)\n",
    "                clients[client_id].save_last_global_model(global_model)\n",
    "\n",
    "    # global model accuracy when all clients finish their training on all tasks (FedCIL ICLR2023)\n",
    "    logger = test_global_model(args, global_test_loader, global_model, logger, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (2, 4), (7, 1), (0, 8), (6, 3)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients[client_id].task_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: [(2, 9), (6, 4), (0, 3), (1, 7), (8, 5)]\n",
      "[[0.5        0.         0.         0.         0.        ]\n",
      " [0.005      0.57249999 0.         0.         0.        ]\n",
      " [0.         0.65249997 0.         0.         0.        ]\n",
      " [0.         0.50749999 0.         0.29249999 0.        ]\n",
      " [0.0825     0.66749996 0.         0.0875     0.55250001]]\n",
      "Final client accuracy: 0.2779999926686287\n",
      "Final client forgetting: 0.15187500230967999\n",
      "Final client balanced accuracy: 0.278\n",
      "\n",
      "Client 1: [(4, 1), (5, 0), (7, 2), (3, 6), (9, 8)]\n",
      "[[0.85249996 0.         0.         0.         0.        ]\n",
      " [0.69499999 0.         0.         0.         0.        ]\n",
      " [0.4025     0.         0.32999998 0.         0.        ]\n",
      " [0.11       0.         0.         0.5025     0.        ]\n",
      " [0.26249999 0.         0.02       0.47749999 0.27250001]]\n",
      "Final client accuracy: 0.20649999752640724\n",
      "Final client forgetting: 0.23124999087303877\n",
      "Final client balanced accuracy: 0.20650000000000004\n",
      "\n",
      "Client 2: [(5, 4), (1, 2), (9, 6), (7, 0), (3, 8)]\n",
      "[[0.5625     0.         0.         0.         0.        ]\n",
      " [0.4975     0.05       0.         0.         0.        ]\n",
      " [0.0675     0.         0.4975     0.         0.        ]\n",
      " [0.21499999 0.015      0.44749999 0.0975     0.        ]\n",
      " [0.29749998 0.         0.26249999 0.0125     0.57999998]]\n",
      "Final client accuracy: 0.23049999102950097\n",
      "Final client forgetting: 0.15875000599771738\n",
      "Final client balanced accuracy: 0.2305\n",
      "\n",
      "Client 3: [(3, 8), (4, 9), (2, 6), (0, 1), (5, 7)]\n",
      "[[0.53749996 0.         0.         0.         0.        ]\n",
      " [0.005      0.505      0.         0.         0.        ]\n",
      " [0.         0.45999998 0.1575     0.         0.        ]\n",
      " [0.085      0.35499999 0.36499998 0.26249999 0.        ]\n",
      " [0.005      0.3425     0.0225     0.         0.57499999]]\n",
      "Final client accuracy: 0.18899999763816594\n",
      "Final client forgetting: 0.32499998179264367\n",
      "Final client balanced accuracy: 0.189\n",
      "\n",
      "Client 4: [(9, 5), (2, 4), (7, 1), (0, 8), (6, 3)]\n",
      "[[0.61500001 0.         0.         0.         0.        ]\n",
      " [0.005      0.5        0.         0.         0.        ]\n",
      " [0.01       0.44749999 0.17       0.         0.        ]\n",
      " [0.015      0.46249998 0.0375     0.22       0.        ]\n",
      " [0.05       0.24249999 0.06       0.1175     0.47499999]]\n",
      "Final client accuracy: 0.18899999633431436\n",
      "Final client forgetting: 0.25875000562518835\n",
      "Final client balanced accuracy: 0.189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for client_id in range(args.n_clients):\n",
    "    print(f'Client {client_id}: {clients[client_id].task_list}')\n",
    "    print(np.mean(logger['test']['acc'][client_id], 0))\n",
    "    print(f'Final client accuracy: {np.mean(np.mean(logger[\"test\"][\"acc\"][client_id], 0)[args.n_tasks-1,:], 0)}')\n",
    "    print(f'Final client forgetting: {np.mean(logger[\"test\"][\"forget\"][client_id])}')\n",
    "    print(f'Final client balanced accuracy: {np.mean(logger[\"test\"][\"bal_acc\"][client_id])}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training results\n",
    "save_results(args, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show images in the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from utils.data_loader import get_statistics\n",
    "\n",
    "\n",
    "mean, std, n_classes, inp_size, in_channels = get_statistics(args)\n",
    "\n",
    "invTrans = transforms.Compose([ transforms.Normalize(mean = np.dot(0, mean),\n",
    "                                                     std = np.divide(1, std)),\n",
    "                                transforms.Normalize(mean = np.dot(-1, mean),\n",
    "                                                     std = np.divide(std, std)),\n",
    "                               ])\n",
    "\n",
    "def show_images(args, imgs, class_id):\n",
    "    dir_plot = f'./images/{args.dataset_name}/{args.memory_size}/{args.uncertainty_score}/{args.balanced_step}/{class_id}'\n",
    "    if not os.path.exists(dir_plot):\n",
    "        os.makedirs(dir_plot)\n",
    "\n",
    "    n_rows = len(imgs) // 10\n",
    "\n",
    "    if n_rows > 1:\n",
    "        fix, axs = plt.subplots(nrows=n_rows, ncols=10, squeeze=False, figsize=(5, n_rows/2))\n",
    "        for n_row in range(n_rows+1):\n",
    "            for n_col in range(10):\n",
    "                img_idx = n_col + n_row * 10\n",
    "                if img_idx == len(imgs): break\n",
    "                img = transforms.ToPILImage()(invTrans(imgs[img_idx]).to('cpu'))\n",
    "                axs[n_row, n_col].imshow(np.asarray(img))\n",
    "                axs[n_row, n_col].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    else:\n",
    "        fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "        for i, img in enumerate(imgs):\n",
    "            img = transforms.ToPILImage()(invTrans(img).to('cpu'))\n",
    "            axs[0, i].imshow(np.asarray(img))\n",
    "            axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "    plt.subplots_adjust(hspace=0, wspace=0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_id = 2 # choose any class_id\n",
    "mem_class = client.memory.x[client.memory.y == class_id]\n",
    "show_images(args, mem_class, class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LT-version of CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torchvision import datasets, transforms\n",
    "from utils.data_loader import get_data_per_class\n",
    "\n",
    "data_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "train = datasets.CIFAR10('./data/raw/', train=True,  download=True, transform=data_transforms)\n",
    "max_num = len(train) / args.n_classes\n",
    "imb_factor = 0.1\n",
    "w_per_cls = []\n",
    "for idx in range(args.n_classes):\n",
    "    num = max_num * (imb_factor**(idx/(args.n_classes-1)))\n",
    "    w = num / max_num\n",
    "    w_per_cls.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict = get_data_per_class(args)\n",
    "skip = args.n_classes_per_task\n",
    "\n",
    "for run in range(args.n_runs):\n",
    "    dir_output = f'{args.dir_data}/data_splits/CL/{args.dataset_name}/run{run}/'\n",
    "    loader_fn = f'{dir_output}/{args.dataset_name}_split.pkl'\n",
    "    cls_assignment_fn = f'{dir_output}/{args.dataset_name}_cls_assignment.pkl'\n",
    "    cls_assignment = pickle.load(open(cls_assignment_fn, 'rb'))\n",
    "    print(cls_assignment)\n",
    "\n",
    "    # for each data split (i.e., train/val/test)\n",
    "    ds_out = {}\n",
    "    for name_ds, ds in ds_dict.items():\n",
    "        split_ds = []\n",
    "        for i in range(0, args.n_classes, skip):\n",
    "            w_list = w_per_cls[i:i+skip]\n",
    "            t_list = cls_assignment[i:i+skip]\n",
    "            task_ds_tmp_x = []\n",
    "            task_ds_tmp_y = []\n",
    "            for idx, class_id in enumerate(t_list):\n",
    "                class_x, class_y = ds[class_id]\n",
    "                num_per_class = int(w_list[idx]*len(class_y))\n",
    "                task_ds_tmp_x.append(class_x[:num_per_class])\n",
    "                task_ds_tmp_y.append(class_y[:num_per_class])\n",
    "\n",
    "            task_ds_x = torch.cat(task_ds_tmp_x)\n",
    "            task_ds_y = torch.cat(task_ds_tmp_y)\n",
    "            split_ds += [(task_ds_x, task_ds_y)]\n",
    "        ds_out[name_ds] = split_ds\n",
    "\n",
    "    ds_list = [ds_out['train'], ds_out['val'], ds_out['test']]\n",
    "    loader_list = []\n",
    "    for ds in ds_list:\n",
    "        loader_tmp = []\n",
    "        for task_data in ds:\n",
    "            images, label = task_data\n",
    "            indices = torch.from_numpy(np.random.choice(images.size(0), images.size(0), replace=False))\n",
    "            images = images[indices]\n",
    "            label = label[indices]\n",
    "            task_ds = torch.utils.data.TensorDataset(images, label)\n",
    "            task_loader = torch.utils.data.DataLoader(task_ds, batch_size=args.batch_size, drop_last=True)\n",
    "            loader_tmp.append(task_loader)\n",
    "        loader_list.append(loader_tmp)\n",
    "\n",
    "    dir_output = f'{args.dir_data}/data_splits/CL/{args.dataset_name}LT/run{run}/'\n",
    "    loader_fn = f'{dir_output}/{args.dataset_name}LT_split.pkl'\n",
    "    cls_assignment_fn = f'{dir_output}/{args.dataset_name}LT_cls_assignment.pkl'\n",
    "    if not os.path.exists(loader_fn):\n",
    "        os.makedirs(dir_output)\n",
    "\n",
    "    # save data splits and cls_assignment\n",
    "    with open(loader_fn, 'wb') as outfile:\n",
    "        pickle.dump(loader_list, outfile)\n",
    "        outfile.close()\n",
    "    with open(cls_assignment_fn, 'wb') as outfile:\n",
    "        pickle.dump(cls_assignment, outfile)\n",
    "        outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(0):\n",
    "    a += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
